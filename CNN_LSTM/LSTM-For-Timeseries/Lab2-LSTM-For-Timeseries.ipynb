{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\" alt=\"Header\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Predictive Maintenance using NVIDIA RAPIDS and Deep Learning Models</h1>\n",
    "<h4 align=\"center\">Part 2: Training GPU LSTM models using Keras+Tensorflow for Time Series</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, we utilized accelerated XGBoost for classification of faulty hard drives. In this lab, we deal with data points gathered from hard drives as time series and use recurrent neural networks to predict hard disk failures. \n",
    "\n",
    "Companies are looking at implementing \"Predictive Maintenance Procedures\" to avoid downtimes by predicting the failure of a device before it happens so they can repair/replace the device, preventing costly downtimes.  \n",
    "\n",
    "In this exercise, we'll leverage the Backblaze Hard Drive SMART data to train an LSTM model that will predict potential future failures.  Unlike the previous exercise, where the XGBoost model predicted based on current data, the LSTM model will leverage time series data to look for trends prior to a failure.\n",
    "\n",
    "One observation that comes from this exercise is that most of the work to train this model revolves around the data preparation.  We will walk through creating the \"normal\" and \"failing\" sequences that will indicate when we expect to see early warning signs of a disk failure.  Then we will use that data to train our model.\n",
    "\n",
    "This lab covers the following topics:\n",
    "\n",
    "* [Recurrent Networks: Long Short-Term Memory](#1)\n",
    "* [Vanishing Gradient Problem and LSTMs](#2)\n",
    "* [LSTM Cell Structure](#3)\n",
    "* [LSTM Applications](#4)\n",
    "* [Preparing the Environment](#5)\n",
    "* [Load Datasets for Data Preparation](#6)\n",
    "* [Preparing the Training Data](#7)\n",
    "* [Creating X and y labels](#8)\n",
    "    * [ Exercise 1: Create class labels](#e1)\n",
    "    * [ Exercise 2: Reshaping the training data](#e2)\n",
    "* [Model Design](#9)\n",
    "* [Training the Model](#10)\n",
    "* [Confusion Matrix](#11)\n",
    "* [Optional:  CNN LSTM Architecture](#12)\n",
    "\n",
    "Let's first discuss LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Recurrent Networks: Long Short-Term Memory\n",
    "\n",
    "In this section, we present a brief overview of Recurrent Neural Networks (RNNs) and the Long short-term memory (LSTM) variation of RNNs, which is used in this lab to predict hard disk failures. Note that an in-depth understanding of internal operations of LSTMs is not required to follow the application presented. However, at the end of this section you are provided with external links for more in-depth analysis of such models and we encourage the students to read through them in order to have a better grasp of the architecture behind RNN networks.\n",
    "\n",
    "RNNs were created to solve the memory-persistence issue found in conventional feed forward networks. Inspired by the human mind, which does not initiate the thinking process from scratch and instead uses memory-persistence based on sequential order of events, RNNs were devised to possess such a characteristic. To accomplish this, RNNs maintain a loop that allows the information to be persistent in the network. Figure 1 depicts an RNN network where at each time step $t$, the network receives input $X_t$ and outputs a value of $y_t$. The left side of the figure shows an RNN cell with input $X$ and output $y$. There is also a loop-back of the state, which represents the memory. At a given timestep $t$, the output also depends on all previous timesteps $t-1, t-2, ..., 0$. This can also be thought of as an unrolled network with each timestep being an input to a different cell and horizontal connections between cells. These connections represent the state that is passed on to the next timestep. Each timestep generates its own output that can be fed into another layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lstm seq.png\" style=\"margin-top:10px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 1. The unrolled recurrent neural network</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the conventional RNNs tend to look at information stored a long time ago, often we are interested in more recent data points, which in most cases suffice to predict the next state. Another well-studied issue with RNNs is the gradient vanishing-exploding problem. In a multi-layer network, gradients are calculated as a product of gradients of many activation functions. Consequently, when those gradients are close to zero, the gradient vanishes. On the other hand, when the previous gradients are bigger, it explodes. We will return to this topic later in this section.\n",
    "\n",
    "__LSTM__ is a type of Recurrent Neural Network specially designed to prevent the neural network output for a given input from either decaying or exploding as it cycles through the feedback loops. The feedback loops are what allow recurrent networks to be better at pattern recognition than other neural networks. \n",
    " \n",
    "Length of memory of past input is critical for solving sequence learning tasks and Long short-term memory networks provide better performance compared to other RNN architectures by alleviating what is called the vanishing gradient problem.\n",
    "\n",
    "Similar to the RNN model shown above, the LSTM Architecture consists of linear units with a self-connection that allows flow of a value (forward pass) or gradient (backward pass) to be preserved and subsequently retrieved at the required time step. The self-recurrent unit, the memory cell, is capable of storing information which lies a dozen of time-steps in the past. This is very powerful for many tasks. For example, for text data, an LSTM unit can store information contained in the previous paragraph and apply this information to a sentence in the current paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## Vanishing Gradient Problem and LSTMs\n",
    "\n",
    "A simple RNN model only has a single hidden RNN layer while a stacked RNN model (needed for advanced applications) has multiple RNN hidden layers. A common problem in deep networks is the “vanishing gradient” problem, where the gradient gets smaller and smaller with each layer until it is too small to affect the deepest layers. \n",
    "\n",
    "This is because small gradients or weights (values less than 1) are multiplied many times over through the multiple time steps, and the gradients shrink asymptotically to zero. This means the weights of those earlier layers won’t be changed significantly and therefore the network won’t learn long-term dependencies.\n",
    "\n",
    "The gradients of neural networks are found during backpropagation.  When we multiply lots of -1 << real numbers << 1 together, we end up with a vanishing product, which leads to a very small value and hence practically no learning of the weight values – the predictive power of the neural network then plateaus.\n",
    "\n",
    "With the memory cell in LSTMs, we have continuous gradient flow (errors maintain their value) which thus eliminates the vanishing gradient problem and enables learning from sequences which are hundreds of time steps long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM models are powerful enough to learn the most important past behaviors and understand whether or not those past behaviors are important features in making future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## LSTM Cell Structure\n",
    "\n",
    "An LSTM cell block is made of of various components called the cell state, the input gate, the forget gate, and the output gate.  The cell state runs along the entire length of the cell and interacts with each of the gates.  The gates are a way to control the information that gets through to the cell state.\n",
    "\n",
    "Here is a graphical representation of the LSTM cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/LSTM cell.png\" style=\"margin-top:10px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 2. LSTM Cell</p> <p style=\"text-align: center;color:gray;font-size:12px;\"> Image from book: <i>Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</i>\n",
    " by Aurélien Géron </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice first, on the bottom, we have our new sequence value $x_t$ being concatenated to the previous output from the cell $h_{t-1}$. The first step for this combined input is for it to be squashed via a $tanh$ layer. Next the output of the $tanh$ operation is passed through an input gate. An input gate is a layer of sigmoid activated nodes whose output is multiplied by the squashed input. These gate sigmoids can act to “kill off” any elements of the input vector that aren’t required and need to be forgotten. A sigmoid function outputs values between 0 and 1, so the weights connecting the input to these nodes can be trained to output values close to zero to “switch off” certain input values (or, conversely, outputs close to 1 to “pass through” other values).\n",
    "\n",
    "The next step in the flow of data through this cell is the internal state / forget gate loop. LSTM cells have an internal state variable $c_t$. This variable, lagged one time step i.e. $c_{t-1}$ is added to the input data to create an effective layer of recurrence. This addition operation, instead of a multiplication operation, helps to reduce the risk of vanishing gradients. However, this recurrence loop is controlled by a forget gate. This works the same as the input gate, but instead helps the network learn which state variables should be “remembered” or “forgotten”.\n",
    "\n",
    "As the final gate, we have an output layer $tanh$ squashing function (re-scale the numbers between -1 and 1), the output of which is controlled by an output gate. This gate determines which values are actually allowed as an output from the cell $h_t$.\n",
    "\n",
    "Last, running along the top of the cell is the cell state, that leverage pointwise multiplication operations to interact with each of the gates.  \n",
    "\n",
    "\n",
    "To have a more in-depth understanding of LSTM networks and relative analysis, we recommend the curious audience to read the following articles:\n",
    "\n",
    " - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    " \n",
    " - [Tutorial for Implementing Long Short-Term Memory](https://heartbeat.fritz.ai/a-beginners-guide-to-implementing-long-short-term-memory-networks-lstm-eb7a2ff09a27)\n",
    " \n",
    " - [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## LSTM Applications\n",
    "\n",
    "LSTMs, due to their ability to learn long term dependencies, are applicable to a number of sequence learning problems including language modeling and translation, acoustic modeling of speech, speech synthesis, speech recognition, audio and video data analysis, handwriting recognition and generation, sequence prediction, and protein secondary structure prediction.\n",
    "\n",
    "In this lab, we apply the LSTM model to the Backblaze data we introduced in the previous lab to create a model that predicts the hard disk failure based on the recorded data-points at least a few days before the actual failure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## Preparing the Environment\n",
    "\n",
    "Below we are importing the required libraries and tools we use throughout this notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from random import shuffle, randrange\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "from progressbar import printProgressBar\n",
    "\n",
    "from alt_model_checkpoint.tensorflow import AltModelCheckpoint\n",
    "\n",
    "harddrive_model = 'ST4000DM000'\n",
    "\n",
    "data_dir = './data/'\n",
    "\n",
    "# Files from Lab 1\n",
    "train_pkl_file = 'Lab1-2017-Full-ST4000DM000.pkl'\n",
    "test_pkl_file = 'Lab1-2016-Q4-ST4000DM000.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TF logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "gpu = 2    # set to number of GPU we want to leverage for training\n",
    "\n",
    "# prevent keras/tf from allocating all gpu memory\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.device('/device:GPU:%s' % gpu)\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## Load Datasets for Data Preparation\n",
    "\n",
    "For your convenience, we have filtered and stored the `ST4000DM000` model datapoints in a pickle file. In the following cell, we are reloading the pickle file into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our training and test set from previous step\n",
    "print(\"Reading in training and test SMART data stored in previous exercise...\")\n",
    "df_train = pd.read_pickle(data_dir+train_pkl_file)\n",
    "df_test = pd.read_pickle(data_dir+test_pkl_file)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to further process the data before the training phase. Each \"raw\" data column has a duplicated \"normalized\" version (example. smart_1_raw and smart_2_normalized) and while values of the pairs are different because of normalization, they do not possess extra information. Similar to the previous lab, we choose to eliminate the \"normalized\" columns.\n",
    "\n",
    "We also need to sort the columns based on the `serial_number` and `date` columns. Since we are dealing with time series data, the only meaningful way to process is to have them sorted, first by the related serial_number and second by the date recorded. The `prepare_dataframe` function is intended to accomplish these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    print(\"Removing unnecessary columns\")\n",
    "    \n",
    "    # find and drop all normalized columns\n",
    "    cols = [c for c in df.columns if (c.lower().find(\"normalized\")!=-1)]\n",
    "    df=df.drop(columns=cols)\n",
    "\n",
    "    # also drop the model and capacity columns as we don't need them\n",
    "    df = df.drop(columns=['model','capacity_bytes'])\n",
    "\n",
    "    # convert string dates to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # sort data by serial number and dates so ready for sequence creation\n",
    "    print(\"Sorting the data frame based on serial number and date\")\n",
    "    df = df.sort_values(by=['serial_number', 'date'], axis=0, ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # ensure no NaN's in data\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the preparation step over both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Processing train set...\")\n",
    "df_train = prepare_dataframe(df_train)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Processing test set...\")\n",
    "df_test = prepare_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences from data sets\n",
    "\n",
    "In this section, we create sequences of data based on the serial numbers. To have a better understanding of this process, we need to elaborate the architecture of the deep learning model that consumes the data. We start with a high level architecture at this section and later we will elaborate the model in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/lstm_simple.png\" style=\"margin-top:10px;width:600px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 3.  basic RNN model. </p>\n",
    "\n",
    "In figure 3, a simplified version of the RNN architecture is shown. Here we have `n` time steps representing each data point in the DataFrame. Also, only the last RNN (LSTM) cell is outputting the value $y$, which in our model indicates whether the data sequence ends up in a failure state or not.\n",
    "Note that the input values $x_1, x_2, …, x_n$ are multi-dimensional values and not single scalars.\n",
    "\n",
    "One important question is how to choose the value $n$. Our dataset contains lengthy time series sequences for each hard drive. Below, we choose a sequence length of _5_ and later ask the students to experiment with other values and justify their value of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create 'sequences' for training our Time Series models, we'll first sort the data by serial number and date (which we did above).  This will allow us to grab groups of sequential records to create each sequence.\n",
    "\n",
    "Normally, we'd create sequences from all of the data that we have.  Even with the small data sets that we have, that can take more than 4 hours.  For this lab, we'll simply the process to make the time reasonable.\n",
    "\n",
    "Failed Records:\n",
    "\n",
    "First, we'll create failed record sequences.  To do this, we'll find all of the failed disk records.  Then, we'll take the previous $sequence length-1$ records and combine it with the failed record to create a sequence of records that ends with a failure.  In the case of our data set, there are so few failed disks (~1600) that we'll use all of them to train our model.\n",
    "\n",
    "Normal Records:\n",
    "\n",
    "For normal records, we'll first identify all disks (by serial_number) that did not have a failure.  Then, we'll select a subset of the disks (to shorten the time) to create sequences for.  To create the sequence, we'll walk sequentially through the records for a given disk and create sequences from that record and the next $sequence length-1$ records.  So, our first sequence (for length 5) would be Day 1-Day 5.  Then, we'll go to the next day and create a sequence of Day 2-Day6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify key parameters - you can change and re-run to test different configurations\n",
    "\n",
    "sequence_length = 5          # Number of days in sequence to train to detect and predict failure\n",
    "\n",
    "num_normal_disks = 20        # Maximum number of normal disks to look at\n",
    "max_normal_sequences = 4000  # Maximum number of normal sequences to create\n",
    "max_failed_sequences = 2000  # Maximum number of failed sequences to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to return serial numbers of good and bad disks\n",
    "\n",
    "def get_disk_serials(df, num_disks):\n",
    "\n",
    "    # Get failed serial numbers\n",
    "    failed_serials = df[df['failure'] == 1]['serial_number'].drop_duplicates().values\n",
    "\n",
    "    # Get serial numbers for disks that didn't fail - first remove failed disks\n",
    "    df_tmp = df[~df.serial_number.isin(failed_serials)]\n",
    "    normal_serials = df_tmp.serial_number.value_counts()[:num_disks].index.tolist()\n",
    "\n",
    "    print('Normal Disk Serials:',len(normal_serials))\n",
    "    print('Failed Disk Serials:',len(failed_serials))\n",
    "\n",
    "    return normal_serials, failed_serials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to return failed sequences\n",
    "\n",
    "def create_failed_sequences(df, sequence_length, num_failed):\n",
    "\n",
    "    # Create dataframe\n",
    "    failed_sequences = pd.DataFrame()\n",
    "    failed_recs = df[df['failure'] == 1].index.tolist()\n",
    "\n",
    "    # Create failed sequences\n",
    "    failed_seq_found = 0\n",
    "    \n",
    "    for failed_rec in failed_recs:\n",
    "        \n",
    "        # if we haven't created enough sequences, add new sequence\n",
    "        if (failed_seq_found < num_failed):\n",
    "        \n",
    "            # Identify start and end of sequence\n",
    "            idx1 = failed_rec-sequence_length+1\n",
    "            idx2 = failed_rec+1\n",
    "\n",
    "            # Append record to failed_sequences\n",
    "            df_tmp = df.iloc[idx1:idx2].copy()\n",
    "            failed_sequences = failed_sequences.append(df_tmp)\n",
    "        \n",
    "            failed_seq_found += 1\n",
    "\n",
    "    return failed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to pick some serial_numbers and create all sequences from those disks up to num_normal sequences\n",
    "\n",
    "def create_normal_sequences(df, sequence_length, num_normal, normal_serials):\n",
    "\n",
    "    # Create dataframe\n",
    "    normal_sequences = pd.DataFrame()\n",
    "    num_seq = 0\n",
    "\n",
    "    # iterate through all serials of good disks\n",
    "    for serial in normal_serials:\n",
    "\n",
    "        # create view of just records for this disk\n",
    "        df_tmp = df[df['serial_number'] == serial]\n",
    "\n",
    "        # count number of records for this disk\n",
    "        num_recs = df_tmp.shape[0]\n",
    "\n",
    "        # iterate through creating all sequences for this serial\n",
    "        for i in range(num_recs-sequence_length+1):\n",
    "            \n",
    "            # if we haven't created enough sequence, add new sequence\n",
    "            if (num_seq < num_normal):\n",
    "                normal_sequences = normal_sequences.append(df_tmp.iloc[i:i+sequence_length,:])\n",
    "                num_seq += 1\n",
    "\n",
    "    return normal_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pools of serial numbers for good and bad disks\n",
    "print(\"Determining good and bad disk serial numbers\")\n",
    "normal_serials, bad_serials = get_disk_serials(df_train, num_normal_disks)\n",
    "\n",
    "# get sequences\n",
    "print(\"Creating sequences for normal disks\")\n",
    "normal_sequences = create_normal_sequences(df_train, sequence_length, max_normal_sequences, normal_serials)\n",
    "print(\"Normal sequences shape:\", normal_sequences.shape)\n",
    "\n",
    "print(\"Creating sequences for failed disks\")\n",
    "failure_sequences = create_failed_sequences(df_train, sequence_length, max_failed_sequences)\n",
    "print(\"Failed sequences shape:\", failure_sequences.shape)\n",
    "\n",
    "# combine to creating training set\n",
    "train_samples = pd.concat([normal_sequences, failure_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few normal disk sequences.  You should see 'sequence_length' records in a row that are sequential days for the same disk.  You'll see the first sequence contains day 1-day 5 (for sequence_length 5) and the second sequence contains day 2-day 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sequences.head(2*sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, lets look at a few failed sequences.  You should see 'sequence_length' record for a given serial number where the last day indicates a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_sequences.head(2*sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check samples\n",
    "num_seq = int(train_samples.shape[0] / sequence_length)\n",
    "print('Found:', num_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objects are huge and in order to prevent memory leaks, we are going to store the data, clear the objects, and retrieve them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store filesnames for later use\n",
    "lab2_train_file = data_dir + 'Lab2-LSTM_train_data_' + str(sequence_length) + '.pkl'\n",
    "lab2_test_file =  data_dir + 'Lab2-LSTM_test_data_' + str(sequence_length) + '.pkl'\n",
    "\n",
    "# Save away training data\n",
    "train_samples.to_pickle(lab2_train_file)\n",
    "del (train_samples)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat the same process for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get pools of serial numbers for good and bad disks\n",
    "print(\"Determining good and bad disk serial numbers\")\n",
    "normal_serials, bad_serials = get_disk_serials(df_test, num_normal_disks)\n",
    "\n",
    "# get sequences\n",
    "print(\"Creating sequences for normal disks\")\n",
    "normal_sequences = create_normal_sequences(df_test, sequence_length, max_normal_sequences, normal_serials)\n",
    "print(\"Normal sequences shape:\", normal_sequences.shape)\n",
    "\n",
    "print(\"Creating sequences for failed disks\")\n",
    "failure_sequences = create_failed_sequences(df_test, sequence_length, max_failed_sequences)\n",
    "print(\"Failed sequences shape:\", failure_sequences.shape)\n",
    "\n",
    "# combine to creating training set\n",
    "test_samples = pd.concat([normal_sequences, failure_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created samples for testing, let's store them away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store away training and test samples\n",
    "test_samples.to_pickle(lab2_test_file)\n",
    "del (test_samples)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## Creating X and y labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to cleanly read the LSTM training and test data sets in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read datasets back in cleanly\n",
    "df_train = pd.read_pickle(lab2_train_file)\n",
    "df_test = pd.read_pickle(lab2_test_file)\n",
    "\n",
    "df_train.head(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating X and y labels, there are certain columns that we need to get rid of. As an example, we can safely remove the `date` column. While the date column values helped us creating the sequences, they cannot be used for training/inference purposes (why?).\n",
    "\n",
    "Also, the `serial_number` and `failure` columns will not be used. Discuss why these two columns are not required."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Discuss your answers here:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"e1\"></a>\n",
    "### Exercise 1: Create class labels\n",
    "\n",
    "To train the model, we need to have the class labels for each sample. Complete the \"TODO\" sections below to store the class labels for `df_train` and `df_test` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# index data will hold disk serial numbers for relevant samples. We do not need serial numbers as training or testing features\n",
    "drop_columns_list = ['date','serial_number', 'failure']\n",
    "\n",
    "x_train = df_train.drop(columns=drop_columns_list)\n",
    "x_test = df_test.drop(columns=drop_columns_list)\n",
    "\n",
    "y_train = <<< TO DO >>>\n",
    "y_test = <<< TO DO >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, click [here](#a1) for an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we removed the normalized columns and kept the raw ones. The reason behind this was that the normalized columns are generated based on much more samples from the original set and hence, do not represent the distribution accurately. In this section, we are going to normalize values in both test and train sets. We will utilize [preprocessing.StandardScaler().fit()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) method for this purpose, but first we need to convert the Pandas DataFrame to NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X train data\n",
    "x_train = x_train.values\n",
    "x_train = x_train.astype(np.float64)\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "# X test data\n",
    "x_test = x_test.values\n",
    "x_test = x_test.astype(np.float64)\n",
    "scaler = preprocessing.StandardScaler().fit(x_test)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"e2\"></a>\n",
    "### Exercise 2: Reshaping the training data\n",
    "\n",
    "Next, we need to reshape the array to get 3D data (num_samples, timesteps, num_features) needed to feed LSTM. Fix to TODO sections below to reshape the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reshaping training and test data for LSTM\")\n",
    "print(\"- Original Train Shape:\", x_train.shape)\n",
    "\n",
    "x_train = <<< TO DO >>>\n",
    "x_test = <<< TO DO >>>\n",
    "\n",
    "print(\"- Final Train Shape:   \", x_train.shape)\n",
    "data_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, click [here](#a2) for an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the y labels (failed or not) for both test and train sets (last column of `y_train` and `y_test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y train data\n",
    "y_train = y_train.values[sequence_length - 1::sequence_length]\n",
    "\n",
    "# Y test data\n",
    "y_test = y_test.values[sequence_length - 1::sequence_length]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## Model Design\n",
    "We've finally arrived at the neural network design! Previously, we provided a simplistic overview of the network. In this section, we are going to discuss the full network design first and then build the model using Keras.\n",
    "\n",
    "In figure 3, we showed how we utilize the output of the last LSTM node in the sequence to create the relevant output. In reality, one-layer LSTM nodes are barely enough to capture the information encoded within a sequence. Alternatively, [stacked LSTMs](https://machinelearningmastery.com/stacked-long-short-term-memory-networks/) allow capturing more data complexity. In our model, we are going to use three layers of stacked LSTMs as shown in figure 4.\n",
    "\n",
    "\n",
    "<img src=\"img/lstm_stacked.png\" style=\"margin-top:10px;width:600px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 4.  Stacked LSTM layers. </p>\n",
    "\n",
    "There is an important difference between the bottom two layers and the top one. While all cells of the bottom two layers send their outputs to their upper layer, the top most layer has only one output which is used for classification purposes. This is achieved by setting the `return_sequences` in the LSTM definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters to control model\n",
    "dp_lvl = 0.2\n",
    "regularizer_lvl = 0.002\n",
    "units = 64\n",
    "# network design\n",
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(x_train.shape[1], x_train.shape[2]),dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  True ))\n",
    "model.add(LSTM(units, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  True ))\n",
    "model.add(LSTM(units, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences =  False ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `units` variable holds the number of output dimensionality for each cell. At the next level, we add three fully-connected (FC) layers. The first FC layer is followed by a [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout). The [Dropout method](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/) is a regularization technique to reduce the possibility of overfitting during training. Using this method, some randomly selected neurons are ignored during training. During the forward pass, the selected neurons do not contribute to the activation function and their weights are also not updated within the backward phase. The last layer has a single output which indicates whether the output of the sequence is a faulty hard drive or not. Let's add those three layers to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "\n",
    "# Dropout is a technique used to tackle Overfitting.\n",
    "model.add(Dropout (0.2))\n",
    "model.add(Dense(units, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "model.add(Dense(1, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, our model is complete and ready to train. Let's first take an overall look at the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"10\"></a>\n",
    "## Training the Model\n",
    "\n",
    "\n",
    "There are a few parameters that we need to define before proceeding with the training. We set the [learning rate](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/), number of epochs, checkpoint file locations, [decay](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1), and more importantly define a numerical optimizer, [Adam](https://arxiv.org/abs/1412.6980), to train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_num = 40\n",
    "learning_rate = 0.001\n",
    "decay_rate = 3 * learning_rate / epochs_num\n",
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None,decay = decay_rate)\n",
    "\n",
    "try:\n",
    "    gpu_model = multi_gpu_model(model, gpus=gpu, cpu_merge=False)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    gpu_model = model\n",
    "    print(\"Training using single GPU\")\n",
    "\n",
    "gpu_model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae'])\n",
    "\n",
    "# checkpoint\n",
    "model_fn = \"best_model.h5\"\n",
    "model_filepath=data_dir+model_fn\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined and data loaded, let's go ahead and train the model.\n",
    "\n",
    "Remember - Our model is learning the sequences that lead up to failure a few days before the failure occurs (imposed through the way data is labeled).  This can be used as guidance on when we might perform preventative maintenance.  In our case, that would be replacing a disk driver prior to an expected failure.\n",
    "\n",
    "<span style=\"background-color:red;color:white;padding:1px,3px\"><b>Note:</b></span> At the beginning, the weights are initialized randomly. Sometimes, specific weight initialization and network settings lead to an early overfitting. Since our training data is small, this may occur more often. If you see an early convergence of accuracy to 1.00, e.g.:\n",
    "\n",
    "```\n",
    "Epoch 00002: val_acc did not improve from 1.00000\n",
    "```\n",
    "\n",
    "you need to start the training over. In the above example, notice that the epoch number 2 is still very early in the training and the network has already overfitted.\n",
    "\n",
    "Now start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "with tf.device('/device:GPU:%s' %gpu):\n",
    "    history = gpu_model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1, shuffle=True, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the accuracy level during the training epochs. One point to observe is that at the beginning, the accuracy rate jumps pretty quickly and afterwards fluctuates heavily. This behavior is due to our small and non-diverse training dataset. With a larger dataset which is also more uniformly distributed, you will be able to experience a smoother convergence path and higher accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show plot accuracy changes during training\n",
    "\n",
    "plt.plot(history.history['acc'],'g')\n",
    "plt.plot(history.history['val_acc'],'r')\n",
    "plt.title('accuracy across epochs')\n",
    "plt.ylabel('accuracy level')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to evaluate the trained model against our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights from best model\n",
    "gpu_model.load_weights(model_filepath)\n",
    "\n",
    "# evaluate model against test set\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae']) \n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"11\"></a>\n",
    "## Confusion Matrix - Remainder From the Previous Lab:\n",
    "\n",
    "---\n",
    "In this section, we are going to plot the confusion matrix. While accuracy rate provides us with an overall outlook on how well our model is performing, we need more insights into the accuracy. This is especially true since our dataset is small.  We need to compromise some aspects of accuracy rate in favor of others. The `classification_report` provides us with the accuracy stats broken down into `precision`, `recall`, `f1-score`, and `support`. Before introducing these measures, let's define some abbreviations:\n",
    "\n",
    "For brevity we are using the following abbreviations: \n",
    "- __FP__: False Positive\n",
    "- __TP__: True  Positive\n",
    "- __FN__: False Negative\n",
    "- __TN__: True  Negative\n",
    "\n",
    "The first measure is focused on identifying positive cases and is called __recall__. We define recall as the ability of the model to identify all true positive samples of the dataset. In mathematical terms, recall is the ratio of true positives over true positives plus false negatives. By other means, recall tells us, among all the test samples belonging to the output class, how many of them are identified correctly by the model:\n",
    "\n",
    "\\begin{equation*}\n",
    "recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation*}\n",
    "\n",
    "The next measure, is called __precision__ and is the ability of the model to identify the relevant samples only, and is defined as the ratio of true positives over true positives plus false positives:\n",
    "\n",
    "\\begin{equation*}\n",
    "precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Selecting a proper threshold, usually stems from a good balance between the precision and recall values. A well-known measure that provides such a balance is `F1 score`, which is a harmonic mean of precision and recall, and defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "{F_1 \\: score} = 2*\\frac{precision*recall}{precision+recall}\n",
    "\\end{equation*}\n",
    "\n",
    "---\n",
    "\n",
    "One very important measure is the recall rate on defective samples. We are interested in models that retrieve as many defective hard drives as possible. The downside is that the high recall rate usually results in a lower precision rate when the dataset is small. Here we are willing to accept this pitfall in favor of a higher recall rate, given the limitation on the dataset size. We try to predict at least half of the defective hard drives. Let's measure how our model is performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "conf_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(classification_report(conf_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_train)\n",
    "conf_train = y_train.reshape(y_train.shape[0],1)\n",
    "print(classification_report(conf_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"12\"></a>\n",
    "## Optional:  CNN LSTM Architecture\n",
    "\n",
    "In this section, we are going to introduce a more advanced architectures, officially known as a __CNN LSTM Sequence Prediction__ model. This model is designed specifically for sequential prediction with spatial inputs (e.g. videos), however, could be applied to time series prediction as well.\n",
    "\n",
    "The CNN LSTM uses a convolutional neural layer to extract features from the sequence on top of the LSTM layers which also inherently extract features. Since we are dealing with one dimensional time series, we are going to use 1D convolutional network, starting with a kernel size of 12 and 50 filters. We also use a [max pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) layer of size 2 to shrink the data size. Figure 5 shows the architecture of the new model.\n",
    "\n",
    "\n",
    "<img src=\"img/lstm_conv.png\" style=\"margin-top:10px;width:800px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> Figure 5.  CNN LSTM layers. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "dp_lvl = 0.2\n",
    "regularizer_lvl = 0.002\n",
    "data_shape = x_train.shape[1:]\n",
    "\n",
    "kernel_s = 3 # should be smaller than sequence length\n",
    "filter_s = 64\n",
    "\n",
    "# network design\n",
    "model = Sequential()\n",
    "model.add(Conv1D(kernel_size = kernel_s, filters = filter_s, padding='same', input_shape=data_shape, activation='relu'))\n",
    "model.add(Conv1D(kernel_size = kernel_s, filters = filter_s, padding='same' ,activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(filter_s, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences = True ))\n",
    "model.add(LSTM(filter_s, dropout = dp_lvl,recurrent_dropout = dp_lvl, return_sequences = False ))\n",
    "model.add(Dense(filter_s, activation='tanh',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "model.add(Dropout (0.2))\n",
    "model.add(Dense(1, activation='relu',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "\n",
    "# training parameters\n",
    "epochs_num = 40\n",
    "learning_rate = 0.001\n",
    "decay_rate = 3 * learning_rate / epochs_num\n",
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None,decay = decay_rate)\n",
    "\n",
    "try:\n",
    "    gpu_model = multi_gpu_model(model, gpus=gpu, cpu_merge=False)\n",
    "    print(\"Training using multiple GPUs..\")\n",
    "except:\n",
    "    gpu_model = model\n",
    "    print(\"Training using single GPU or CPU..\")\n",
    "\n",
    "# print out model\n",
    "model.summary()\n",
    "\n",
    "# checkpoint\n",
    "model_fn = \"best_1D_model.h5\"\n",
    "model_filepath=data_dir+model_fn\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "es = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [es, checkpoint]\n",
    "\n",
    "gpu_model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model built, let’s train the new model to determine its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "with tf.device('/device:GPU:%s' %gpu):\n",
    "    history = gpu_model.fit(x_train, y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out by reviewing the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot accuracy changes during training\n",
    "\n",
    "plt.plot(history.history['acc'],'g')\n",
    "plt.plot(history.history['val_acc'],'r')\n",
    "plt.title('accuracy across epochs')\n",
    "plt.ylabel('accuracy level')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check out the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights from best model\n",
    "gpu_model.load_weights(model_filepath)\n",
    "\n",
    "# evaluate model against test set\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy', 'mae']) \n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"%s: %.2f%% %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100, model.metrics_names[2], scores[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous model, we are going to build the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix on test set\n",
    "y_pred = model.predict_classes(x_test)\n",
    "conf_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(classification_report(conf_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging the model for calculating \"Remaining Useful Life\" (RUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case above, we developed a \"classifier\" which will indicate, based on SMART drive data, that a given hard drive will fail within the \"sequnce_length\".  By increasing the sequence_length, we could potentially learn to detect failures further in the future.  In additional, different failure types could have different failure windows, so an ensemble of models, each with different windows and sequences could be leveraged for more advanced RUL predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory for upcoming exercises\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What do you think about the model accuracy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Discuss some ways that you could improve the performance of the model above ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"assess\"></a>\n",
    "## Assessment – Predicting Time Series\n",
    "\n",
    "The RNN model we created in this lab was designed to output a single value, indicating whether the subject time series results in a \"failed\" hard drive or not. To accomplish this, a simple class label is returned by the model through the below line of Keras code:\n",
    "\n",
    "```\n",
    "model.add(Dense(1, activation='relu',activity_regularizer=regularizers.l2(regularizer_lvl)))\n",
    "```\n",
    "\n",
    "In this assignment, we intend to perform a slightly different approach. For example, while previously we were given with sequences of length \"5\" (the original value), now we assume that the length of the sequence is one less, e.g. 4 (these are arbitrary lengths for the sake of example) in our case. Predicting the 5th element of the sequence is the subject of our model. By other means, we want the model to return a single (multi-dimensional) vector, (a vector of size [1 x 47] for the above DataFrame. You should determine the vector size by the array shape). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img/Lab2-Assessment-Example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the above criteria, there are some other changes required for the architecture of the model:\n",
    "\n",
    "1- The model starts with two stacked LSTM layers with unit size of 128.\n",
    "\n",
    "2- The last output of the second LSTM layer is fed into a Dense model with the following properties: \n",
    "    - The number of outputs is 256\n",
    "    - The activation function is `tanh`\n",
    "    \n",
    "3- The fourth layer is a dropout layer with dropout probability of 0.2\n",
    "\n",
    "4- Next is another dense model with:\n",
    "    - The number of outputs is 128\n",
    "    - The activation function is `tanh`\n",
    "    \n",
    "5- The last layer is a Dense layer where the number of outputs is equal to the dimension of a single row of data. (Remember that we are trying to predict the values of a multi dimension vector).\n",
    "    \n",
    "The function should simply return the above model.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__To do so, open [this python file](/lab/edit/assignment.py)__  and complete the `get_model()` function by filling in the __<<< TODO >>>__ section according to the criteria described above.\n",
    "\n",
    "\n",
    "Once done, you can use the following block of code to verify that your implementation is correct. Note that this is __NOT__ the last step. After you succeed the test below you need to click on assess test button on the course page. For more details, read the red note below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python try_assessment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:red;color:white;padding:1px,3px\"><b>Note:</b></span> __Once the code is completed, go back to the assessment page (where you clicked on the current notebook) and click on the `ASSESS TASK` button. If you have correctly implemented the code, you will pass this section.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs provide a powerful way to analyze time series data.  While XGBoost was able to predict failure for a given set of SMART data, the LSTM gave us the ability to predict future disk failures before they happen.\n",
    "\n",
    "One thing to remember: The amount of data, model complexity, number of features, and number of epochs have been reduced in this tutorial to reduce the computational (and time) burden.  If we were leveraging this model for production usage, we would train on a much larger data set.  \n",
    "\n",
    "This training would take hours to days and we would regularly retrain the model leveraging additional data as it became available.  Also, a key thing to remember is that the data is device and environment dependent so any major changes would require retraining to ensure the model was still appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"All-answers\"></a>\n",
    "## Answers to selected exercises:\n",
    "---\n",
    "\n",
    "<a name=\"a1\"></a>\n",
    "**Exercise 1: Create class labels:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = df_train['failure']\n",
    "y_test = df_test['failure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](#e1) to go back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a2\"></a>\n",
    "**Exercise 2: Reshaping the training data**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = x_train.reshape(int(x_train.shape[0]/sequence_length), sequence_length, x_train.shape[1])\n",
    "x_test = x_test.reshape(int(x_test.shape[0]/sequence_length), sequence_length, x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](#e2) to go back"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
